{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import requests\n",
    "# import joblib\n",
    "# import json\n",
    "# import logging\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.utils import resample\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# from azureml.core.environment import Environment\n",
    "# from azureml.core.conda_dependencies import CondaDependencies\n",
    "# from azureml.core.experiment import Experiment\n",
    "# from azureml.core.workspace import Workspace\n",
    "# from azureml.core.dataset import Dataset\n",
    "# from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "# from azureml.core.compute import AmlCompute\n",
    "# from azureml.core.compute import ComputeTarget\n",
    "# from azureml.core.compute_target import ComputeTargetException\n",
    "# from azureml.core.model import Model\n",
    "# from azureml.widgets import RunDetails\n",
    "# from azureml.train.automl import AutoMLConfig\n",
    "# from azureml.core.webservice import LocalWebservice\n",
    "# from azureml.core.webservice import Webservice, AciWebservice\n",
    "# from azureml.core.model import InferenceConfig\n",
    "# from azureml.automl.core.shared import constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "The credit approval data is taken from UCI data repository. We will use the data to train the model and predict to approve credit or not.\n",
    "\n",
    "All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. \n",
    "\n",
    "This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "\n",
    "A1: b, a. \\\n",
    "A2: continuous. \\\n",
    "A3: continuous. \\\n",
    "A4: u, y, l, t. \\\n",
    "A5: g, p, gg. \\\n",
    "A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff. \\\n",
    "A7: v, h, bb, j, n, z, dd, ff, o. \\\n",
    "A8: continuous. \\\n",
    "A9: t, f. \\\n",
    "A10: t, f. \\\n",
    "A11: continuous. \\\n",
    "A12: t, f. \\\n",
    "A13: g, p, s. \\\n",
    "A14: continuous. \\\n",
    "A15: continuous. \\\n",
    "A16: +,- (class attribute)\n",
    "\n",
    "Data has 296 approved and 357 rejected applications.\n",
    "\n",
    "This dataset is public and available for research.\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    columns = ['A'+str(i) for i in range(1,17)]\n",
    "    df = pd.read_csv('data.csv', header = None, names = columns )\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    # Removing Null\n",
    "    for column in df.columns:\n",
    "        df = df[df[column] != '?']\n",
    "\n",
    "    # Converting attributes to binary\n",
    "    df['A16'] = df['A16'].map({'+': 1, '-':0})    \n",
    "    df['A1']  = df['A1'].map({'b': 1, 'a':0})\n",
    "    df['A9']  = df['A9'].map({'t': 1, 'f':0})\n",
    "    df['A10'] = df['A10'].map({'t': 1, 'f':0})\n",
    "    df['A12'] = df['A12'].map({'t': 0, 'f':1})\n",
    "\n",
    "    # Conversting categorical data into onehot encoding\n",
    "    cat_columns = ['A4', 'A5', 'A6', 'A7', 'A13']\n",
    "    for column in cat_columns:\n",
    "        dummies = pd.get_dummies(df[column])\n",
    "        df[dummies.columns] = dummies\n",
    "        df.drop(columns = column,inplace = True)\n",
    "    df = df.astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    357\n",
       "1.0    296\n",
       "Name: A16, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['A16'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'your experiment name here'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653, 39)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'credit_approval_data'\n",
    "\n",
    "# Check Registry\n",
    "if dataset_name in ws.datasets.keys(): \n",
    "    dataset = ws.datasets[dataset_name] \n",
    "\n",
    "    print(f'Located {dataset_name} in Registry.')\n",
    "\n",
    "# Read from UCI ML Repository\n",
    "else:\n",
    "    df = get_data()\n",
    "    \n",
    "    # Register the dataset\n",
    "    datastore = ws.get_default_datastore()\n",
    "\n",
    "    dataset = Dataset.Tabular.register_pandas_dataframe(\n",
    "        dataframe=df, \n",
    "#         name=dataset_name, \n",
    "        description='A dataset of credit approved and rejected applications',\n",
    "        target=datastore\n",
    "    )\n",
    "\n",
    "    print(f'Read {dataset_name} from UCI ML Repository and registered dataset.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the DataFrame into training and testing datasets\n",
    "train, test = train_test_split(df, random_state=99, shuffle=True, test_size=0.2)\n",
    "datastore = ws.get_default_datastore()\n",
    "train = Dataset.Tabular.register_pandas_dataframe(\n",
    "        dataframe=train, \n",
    "        name='df_train', \n",
    "        description='training dataset',\n",
    "        target=datastore\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "*The experiment_timeout_minutes (20 mins), iterations (50) and enable_early_stopping to reduce time taken for model training. \n",
    "*Enabling early stopping allows the training process to conclude if there is no considerable improvement to the primary_metric.\n",
    "*cross_validations:5 is set to prevent overfitting.\n",
    "*The max_concurrent_iterations: 4 is used to allow jobs to be run in parallel on each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\":20,\n",
    "    \"enable_early_stopping\":True,    \n",
    "    \"primary_metric\":'accuracy',\n",
    "    \"n_cross_validations\":5,\n",
    "    \"iterations\":50,\n",
    "    \"max_concurrent_iterations\":4,\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(\n",
    "    compute_target=compute_target, \n",
    "    task='classification', \n",
    "    training_data=train,\n",
    "    label_column_name='A16',\n",
    "    **automl_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "remote_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "status = remote_run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "\n",
    "\n",
    "# Best Run\n",
    "print(\"Best Run: \")\n",
    "print(\"\")\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy: \" + str(best_run.get_metrics()['accuracy']))\n",
    "print(\"\")\n",
    "\n",
    "# Metrics\n",
    "print(\"Metrics: \" + str(best_run.get_metrics()))\n",
    "print(\"\")\n",
    "\n",
    "# Name\n",
    "print(\"Name: \" + str(best_run.properties['model_name']))\n",
    "print(\"\")\n",
    "\n",
    "# Tags\n",
    "print(\"Tags: \" + str(best_run.get_tags()))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best model\n",
    "#TODO: Save the best model\n",
    "joblib.dump(fitted_model, filename='automl_best_model.joblib')\n",
    "\n",
    "# Register the best model\n",
    "model_name = best_run.properties['model_name']\n",
    "model = remote_run.register_model(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
